{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from attention import AttentionLayer\n",
    "from os import listdir\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import nltk\n",
    "import pandas as pd \n",
    "import re           \n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, encoding='utf-8')\n",
    "\ttext = file.read()\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "\t# find first highlight\n",
    "\tindex = doc.find('@highlight')\n",
    "\t# split into story and highlights\n",
    "\tstory, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "\t# strip extra white space around each highlight\n",
    "\thighlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "\treturn story, highlights\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "    stories = []\n",
    "    highlights =[]\n",
    "    for name in listdir(directory)[:100]:\n",
    "        filename = directory + '/' + name\n",
    "\t\t# load document\n",
    "        doc = load_doc(filename)\n",
    "\t\t# split into story and highlights\n",
    "        story, highlight = split_story(doc)\n",
    "\t\t# store\n",
    "        stories.append(story)\n",
    "        highlights.append(highlight)\n",
    "    data = pd.DataFrame()\n",
    "    data[\"story\"] = stories\n",
    "    data[\"highlight\"] = highlights\n",
    "    \n",
    "    return  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stories\n",
    "directory = 'dati/cnn/stories/'\n",
    "data = load_stories(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 100\n"
     ]
    }
   ],
   "source": [
    "print('Loaded Stories %d' % len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>highlight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the start of a big week for the Higgs boson, the most sought-after particle in all of physics, scientists in Illinois said Monday that they had crept closer to proving that the particle exists ...</td>\n",
       "      <td>[U.S.-based scientists say their data points toward the existence of the Higgs boson, Finding the Higgs boson would help explain the origin of mass, But the research at the Tevatron collider doesn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN)George Zimmerman -- acquitted by a Florida jury over the death of Trayvon Martin -- was arrested in Florida on suspicion of aggravated assault and domestic violence with a weapon, local autho...</td>\n",
       "      <td>[Zimmerman posts $5,000 bail; he was accused of throwing a bottle at a girlfriend, \"He hasn't been very lucky with the ladies,\" attorney says of Zimmerman, He became a national figure after being ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(CNN) -- Zlatan Ibrahimovic scored his third goal in as many games to continue his return to form and lift Barcelona to the top of Spain's La Liga on Saturday night.\\n\\nThe Sweden striker, who was...</td>\n",
       "      <td>[Barcelona move three points clear of Real Madrid with 1-0 victory at Mallorca, Sweden striker Zlatan Ibrahimovic nets winner for his third goal in a week, Real can return to top on goal differenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(CNN) -- Nobel laureate Norman E. Borlaug, an agricultural scientist who helped develop disease-resistant wheat used to fight famine in poor countries, died Saturday. He was 95.\\n\\nNorman Borlaug ...</td>\n",
       "      <td>[Borlaug died at the age of 95 from complications caused by cancer, In 1970, was awarded the Nobel Peace Prize for his contribution to science, Helped develop disease-resistant wheat, worked to ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)Louisiana Gov. Bobby Jindal on Monday stood by his criticism of so-called \"no-go\" zones in Europe, where sovereign nations allegedly cede authority to Muslim immigrants, a controversial idea ...</td>\n",
       "      <td>[Louisiana Gov. Bobby Jindal decried \"no-go zones,\" where sovereign governments cede authority to Muslims, A Fox News commentator sparked controversy when he mentioned the idea last week, which ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                     story  \\\n",
       "0  At the start of a big week for the Higgs boson, the most sought-after particle in all of physics, scientists in Illinois said Monday that they had crept closer to proving that the particle exists ...   \n",
       "1  (CNN)George Zimmerman -- acquitted by a Florida jury over the death of Trayvon Martin -- was arrested in Florida on suspicion of aggravated assault and domestic violence with a weapon, local autho...   \n",
       "2  (CNN) -- Zlatan Ibrahimovic scored his third goal in as many games to continue his return to form and lift Barcelona to the top of Spain's La Liga on Saturday night.\\n\\nThe Sweden striker, who was...   \n",
       "3  (CNN) -- Nobel laureate Norman E. Borlaug, an agricultural scientist who helped develop disease-resistant wheat used to fight famine in poor countries, died Saturday. He was 95.\\n\\nNorman Borlaug ...   \n",
       "4  (CNN)Louisiana Gov. Bobby Jindal on Monday stood by his criticism of so-called \"no-go\" zones in Europe, where sovereign nations allegedly cede authority to Muslim immigrants, a controversial idea ...   \n",
       "\n",
       "                                                                                                                                                                                                 highlight  \n",
       "0  [U.S.-based scientists say their data points toward the existence of the Higgs boson, Finding the Higgs boson would help explain the origin of mass, But the research at the Tevatron collider doesn...  \n",
       "1  [Zimmerman posts $5,000 bail; he was accused of throwing a bottle at a girlfriend, \"He hasn't been very lucky with the ladies,\" attorney says of Zimmerman, He became a national figure after being ...  \n",
       "2  [Barcelona move three points clear of Real Madrid with 1-0 victory at Mallorca, Sweden striker Zlatan Ibrahimovic nets winner for his third goal in a week, Real can return to top on goal differenc...  \n",
       "3  [Borlaug died at the age of 95 from complications caused by cancer, In 1970, was awarded the Nobel Peace Prize for his contribution to science, Helped develop disease-resistant wheat, worked to ea...  \n",
       "4  [Louisiana Gov. Bobby Jindal decried \"no-go zones,\" where sovereign governments cede authority to Muslims, A Fox News commentator sparked controversy when he mentioned the idea last week, which ha...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.explode(\"highlight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\"you're\": \"you are\", \"you've\": \"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def text_cleaner(input_text):\n",
    "    #strips source cnn office if there\n",
    "    cleanString = input_text.replace('(CNN) -- ', '')\n",
    "    cleanString = cleanString.lower()\n",
    "    cleanString = re.sub(r'\\([^)]*\\)', '', cleanString)\n",
    "    cleanString = re.sub('\"','', cleanString)\n",
    "    cleanString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in cleanString.split(\" \")])    \n",
    "    cleanString = re.sub(r\"'s\\b\",\"\",cleanString)\n",
    "    cleanString = re.sub(\"[^a-zA-Z]\", \" \", cleanString) \n",
    "    tokens = [w for w in cleanString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=2:                  \n",
    "            long_words.append(i)   \n",
    "\n",
    "    return (\" \".join(long_words)).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_story = []\n",
    "for t in data['story']:\n",
    "    cleaned_story.append(text_cleaner(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_cleaner(input_text):\n",
    "    cleanString = input_text.lower()\n",
    "    cleanString = re.sub(r'\\([^)]*\\)', '', cleanString)\n",
    "    cleanString = re.sub('\"','', cleanString)\n",
    "    cleanString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in cleanString.split(\" \")])    \n",
    "    cleanString = re.sub(r\"'s\\b\",\"\",cleanString)\n",
    "    cleanString = re.sub(\"[^a-zA-Z]\", \" \", cleanString) \n",
    "\n",
    "    tokens = [w for w in cleanString.split() if not w in stop_words]\n",
    "    results = []\n",
    "    for i in tokens:            \n",
    "        results.append(i)   \n",
    "\n",
    "    return (\" \".join(results)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the above function\n",
    "cleaned_highlight = []\n",
    "\n",
    "for t in data['highlight']:\n",
    "    highlight_per_story = []\n",
    "    cleaned_highlight.append(highlight_cleaner(t))\n",
    "\n",
    "data['cleaned_text']=cleaned_story\n",
    "data['cleaned_highlight']=cleaned_highlight\n",
    "\n",
    "# Prende spazi vuoti, riempie con NAN e poi leba i NAN\n",
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_highlight'] = data['cleaned_highlight'].apply(lambda x : 'starttoken '+ x + ' endtoken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>highlight</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_highlight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the start of a big week for the Higgs boson, the most sought-after particle in all of physics, scientists in Illinois said Monday that they had crept closer to proving that the particle exists ...</td>\n",
       "      <td>U.S.-based scientists say their data points toward the existence of the Higgs boson</td>\n",
       "      <td>start big week higgs boson sought particle physics scientists illinois said monday crept closer proving particle exists unable reach definitive conclusion scientists outlined final analysis based ...</td>\n",
       "      <td>starttoken u based scientists say data points toward existence higgs boson endtoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the start of a big week for the Higgs boson, the most sought-after particle in all of physics, scientists in Illinois said Monday that they had crept closer to proving that the particle exists ...</td>\n",
       "      <td>Finding the Higgs boson would help explain the origin of mass</td>\n",
       "      <td>start big week higgs boson sought particle physics scientists illinois said monday crept closer proving particle exists unable reach definitive conclusion scientists outlined final analysis based ...</td>\n",
       "      <td>starttoken finding higgs boson would help explain origin mass endtoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the start of a big week for the Higgs boson, the most sought-after particle in all of physics, scientists in Illinois said Monday that they had crept closer to proving that the particle exists ...</td>\n",
       "      <td>But the research at the Tevatron collider doesn't provide a conclusive answer</td>\n",
       "      <td>start big week higgs boson sought particle physics scientists illinois said monday crept closer proving particle exists unable reach definitive conclusion scientists outlined final analysis based ...</td>\n",
       "      <td>starttoken research tevatron collider provide conclusive answer endtoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the start of a big week for the Higgs boson, the most sought-after particle in all of physics, scientists in Illinois said Monday that they had crept closer to proving that the particle exists ...</td>\n",
       "      <td>Attention now turns to a seminar Wednesday on data from the Large Hadron Collider</td>\n",
       "      <td>start big week higgs boson sought particle physics scientists illinois said monday crept closer proving particle exists unable reach definitive conclusion scientists outlined final analysis based ...</td>\n",
       "      <td>starttoken attention turns seminar wednesday data large hadron collider endtoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN)George Zimmerman -- acquitted by a Florida jury over the death of Trayvon Martin -- was arrested in Florida on suspicion of aggravated assault and domestic violence with a weapon, local autho...</td>\n",
       "      <td>Zimmerman posts $5,000 bail; he was accused of throwing a bottle at a girlfriend</td>\n",
       "      <td>george zimmerman acquitted florida jury death trayvon martin arrested florida suspicion aggravated assault domestic violence weapon local authorities said year old florida resident arrested friday...</td>\n",
       "      <td>starttoken zimmerman posts bail accused throwing bottle girlfriend endtoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>New York (CNN) -- A man who allegedly threatened police officers with a knife was shot and fatally wounded Saturday afternoon near Times Square in Manhattan, authorities said.\\n\\nThe 3 p.m. incide...</td>\n",
       "      <td>Area is popular with shoppers, tourists</td>\n",
       "      <td>new york man allegedly threatened police officers knife shot fatally wounded saturday afternoon near times square manhattan authorities said incident occurred area popular shoppers tourists began ...</td>\n",
       "      <td>starttoken area popular shoppers tourists endtoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>London (CNN) -- A journalist at the News of the World, the UK newspaper that was closed by Rupert Murdoch last year, once said: \"This is what we do. We go out and destroy people's lives.\" The Brit...</td>\n",
       "      <td>After Lord Leveson's report on press ethics, James Robinson asks what will change</td>\n",
       "      <td>london journalist news world uk newspaper closed rupert murdoch last year said go destroy people lives british press combative uncompromising feature national life many years high court judge lord...</td>\n",
       "      <td>starttoken lord leveson report press ethics james robinson asks change endtoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>London (CNN) -- A journalist at the News of the World, the UK newspaper that was closed by Rupert Murdoch last year, once said: \"This is what we do. We go out and destroy people's lives.\" The Brit...</td>\n",
       "      <td>Robinson: British press has vowed to clean up its act before, after Diana's death</td>\n",
       "      <td>london journalist news world uk newspaper closed rupert murdoch last year said go destroy people lives british press combative uncompromising feature national life many years high court judge lord...</td>\n",
       "      <td>starttoken robinson british press vowed clean act diana death endtoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>London (CNN) -- A journalist at the News of the World, the UK newspaper that was closed by Rupert Murdoch last year, once said: \"This is what we do. We go out and destroy people's lives.\" The Brit...</td>\n",
       "      <td>PM David Cameron, newspapers and the police are immediate winners, he says</td>\n",
       "      <td>london journalist news world uk newspaper closed rupert murdoch last year said go destroy people lives british press combative uncompromising feature national life many years high court judge lord...</td>\n",
       "      <td>starttoken pm david cameron newspapers police immediate winners says endtoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>London (CNN) -- A journalist at the News of the World, the UK newspaper that was closed by Rupert Murdoch last year, once said: \"This is what we do. We go out and destroy people's lives.\" The Brit...</td>\n",
       "      <td>Tabloid editors and the Murdochs were roundly criticized by the report, he adds</td>\n",
       "      <td>london journalist news world uk newspaper closed rupert murdoch last year said go destroy people lives british press combative uncompromising feature national life many years high court judge lord...</td>\n",
       "      <td>starttoken tabloid editors murdochs roundly criticized report adds endtoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      story  \\\n",
       "0   At the start of a big week for the Higgs boson, the most sought-after particle in all of physics, scientists in Illinois said Monday that they had crept closer to proving that the particle exists ...   \n",
       "0   At the start of a big week for the Higgs boson, the most sought-after particle in all of physics, scientists in Illinois said Monday that they had crept closer to proving that the particle exists ...   \n",
       "0   At the start of a big week for the Higgs boson, the most sought-after particle in all of physics, scientists in Illinois said Monday that they had crept closer to proving that the particle exists ...   \n",
       "0   At the start of a big week for the Higgs boson, the most sought-after particle in all of physics, scientists in Illinois said Monday that they had crept closer to proving that the particle exists ...   \n",
       "1   (CNN)George Zimmerman -- acquitted by a Florida jury over the death of Trayvon Martin -- was arrested in Florida on suspicion of aggravated assault and domestic violence with a weapon, local autho...   \n",
       "..                                                                                                                                                                                                      ...   \n",
       "98  New York (CNN) -- A man who allegedly threatened police officers with a knife was shot and fatally wounded Saturday afternoon near Times Square in Manhattan, authorities said.\\n\\nThe 3 p.m. incide...   \n",
       "99  London (CNN) -- A journalist at the News of the World, the UK newspaper that was closed by Rupert Murdoch last year, once said: \"This is what we do. We go out and destroy people's lives.\" The Brit...   \n",
       "99  London (CNN) -- A journalist at the News of the World, the UK newspaper that was closed by Rupert Murdoch last year, once said: \"This is what we do. We go out and destroy people's lives.\" The Brit...   \n",
       "99  London (CNN) -- A journalist at the News of the World, the UK newspaper that was closed by Rupert Murdoch last year, once said: \"This is what we do. We go out and destroy people's lives.\" The Brit...   \n",
       "99  London (CNN) -- A journalist at the News of the World, the UK newspaper that was closed by Rupert Murdoch last year, once said: \"This is what we do. We go out and destroy people's lives.\" The Brit...   \n",
       "\n",
       "                                                                              highlight  \\\n",
       "0   U.S.-based scientists say their data points toward the existence of the Higgs boson   \n",
       "0                         Finding the Higgs boson would help explain the origin of mass   \n",
       "0         But the research at the Tevatron collider doesn't provide a conclusive answer   \n",
       "0     Attention now turns to a seminar Wednesday on data from the Large Hadron Collider   \n",
       "1      Zimmerman posts $5,000 bail; he was accused of throwing a bottle at a girlfriend   \n",
       "..                                                                                  ...   \n",
       "98                                              Area is popular with shoppers, tourists   \n",
       "99    After Lord Leveson's report on press ethics, James Robinson asks what will change   \n",
       "99    Robinson: British press has vowed to clean up its act before, after Diana's death   \n",
       "99           PM David Cameron, newspapers and the police are immediate winners, he says   \n",
       "99      Tabloid editors and the Murdochs were roundly criticized by the report, he adds   \n",
       "\n",
       "                                                                                                                                                                                               cleaned_text  \\\n",
       "0   start big week higgs boson sought particle physics scientists illinois said monday crept closer proving particle exists unable reach definitive conclusion scientists outlined final analysis based ...   \n",
       "0   start big week higgs boson sought particle physics scientists illinois said monday crept closer proving particle exists unable reach definitive conclusion scientists outlined final analysis based ...   \n",
       "0   start big week higgs boson sought particle physics scientists illinois said monday crept closer proving particle exists unable reach definitive conclusion scientists outlined final analysis based ...   \n",
       "0   start big week higgs boson sought particle physics scientists illinois said monday crept closer proving particle exists unable reach definitive conclusion scientists outlined final analysis based ...   \n",
       "1   george zimmerman acquitted florida jury death trayvon martin arrested florida suspicion aggravated assault domestic violence weapon local authorities said year old florida resident arrested friday...   \n",
       "..                                                                                                                                                                                                      ...   \n",
       "98  new york man allegedly threatened police officers knife shot fatally wounded saturday afternoon near times square manhattan authorities said incident occurred area popular shoppers tourists began ...   \n",
       "99  london journalist news world uk newspaper closed rupert murdoch last year said go destroy people lives british press combative uncompromising feature national life many years high court judge lord...   \n",
       "99  london journalist news world uk newspaper closed rupert murdoch last year said go destroy people lives british press combative uncompromising feature national life many years high court judge lord...   \n",
       "99  london journalist news world uk newspaper closed rupert murdoch last year said go destroy people lives british press combative uncompromising feature national life many years high court judge lord...   \n",
       "99  london journalist news world uk newspaper closed rupert murdoch last year said go destroy people lives british press combative uncompromising feature national life many years high court judge lord...   \n",
       "\n",
       "                                                                      cleaned_highlight  \n",
       "0   starttoken u based scientists say data points toward existence higgs boson endtoken  \n",
       "0                starttoken finding higgs boson would help explain origin mass endtoken  \n",
       "0              starttoken research tevatron collider provide conclusive answer endtoken  \n",
       "0      starttoken attention turns seminar wednesday data large hadron collider endtoken  \n",
       "1           starttoken zimmerman posts bail accused throwing bottle girlfriend endtoken  \n",
       "..                                                                                  ...  \n",
       "98                                   starttoken area popular shoppers tourists endtoken  \n",
       "99      starttoken lord leveson report press ethics james robinson asks change endtoken  \n",
       "99               starttoken robinson british press vowed clean act diana death endtoken  \n",
       "99        starttoken pm david cameron newspapers police immediate winners says endtoken  \n",
       "99          starttoken tabloid editors murdochs roundly criticized report adds endtoken  \n",
       "\n",
       "[354 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_highlight']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9548022598870056\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in data['cleaned_highlight']:\n",
    "    if(len(i.split()) <= 12):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(data['cleaned_highlight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_len = 300\n",
    "max_summary_len = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = np.array(data['cleaned_text'])\n",
    "cleaned_summary = np.array(data['cleaned_highlight'])\n",
    "\n",
    "short_text = []\n",
    "short_summary = []\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_text[i].split()) <= max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "df = pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr,x_val,y_tr,y_val = train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True)\n",
    "#x_tr,x_val,y_tr,y_val=train_test_split(data['cleaned_text'],data['cleaned_highlight'],test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le parole con un conteggio minore di thresh sono considerate rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 44.85870339586796\n",
      "Total Coverage of rare words: 16.204690831556505\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2323"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 99.1944764096663\n",
      "Total Coverage of rare words: 75.19163763066203\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 153)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['starttoken'],len(y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am deleting the rows that contain only START and END tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x7fa1bc77ddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x7fa1bc77ddd0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x7fa1bc77ddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x7fa1bc77ddd0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 300, 100)     232300      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 300, 300), ( 481200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 300, 300), ( 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    800         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 300, 300), ( 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 8)      4808        concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,823,008\n",
      "Trainable params: 2,823,008\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42 samples, validate on 4 samples\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,\n",
    "                  epochs= 5, verbose = 1,\n",
    "                  validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['starttoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "                \n",
    "        if(sampled_token!='endtoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'endtoken'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['starttoken']) and i!=target_word_index['endtoken']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(\"Review:\",seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
