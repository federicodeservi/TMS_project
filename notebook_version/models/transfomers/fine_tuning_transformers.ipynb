{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('nlp': conda)",
      "metadata": {
        "interpreter": {
          "hash": "08da0f045c22bfc9d0ca1ec99ede9ebdd7a7506fb229c3025070bc74466ac14d"
        }
      }
    },
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvkTTztiQoND"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBf9FBoWQoNQ"
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "from os import listdir\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np  \n",
        "import pandas as pd \n",
        "from IPython.display import display, clear_output\n",
        "import re           \n",
        "from bs4 import BeautifulSoup \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords   \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import pickle\n",
        "import inflect\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ThM5ZHZQoNU",
        "outputId": "d2589387-b2bd-4e4a-9a97-4603c5d3cc18"
      },
      "source": [
        "if tf.test.gpu_device_name(): \n",
        "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "   print(\"Please install GPU version of TF\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please install GPU version of TF\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Load data"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"../../preprocessing/test_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"cleaned_highlight\"] = data[\"cleaned_highlight\"].str.replace(\"starttoken\", \"\")\n",
        "data[\"cleaned_highlight\"] = data[\"cleaned_highlight\"].str.replace(\"endtoken\", \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "stories = data[\"cleaned_text\"]\n",
        "summaries = data[\"cleaned_highlight\"]"
      ]
    },
    {
      "source": [
        "# Model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, BartForConditionalGeneration,  BartTokenizer, PegasusForConditionalGeneration, PegasusTokenizer\n",
        "\n",
        "# initialize the model architecture and weights\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "# initialize the model tokenizer\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGmvYD07QoN5"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQUkioFEQoN9"
      },
      "source": [
        "original_text = []\n",
        "original_summary = []\n",
        "created_summary_t5 = []\n",
        "created_summary_bart = []\n",
        "created_summary_pegasus = []\n",
        "\n",
        "for i in range(0,2):\n",
        "    clear_output(wait=True)\n",
        "    print(i)\n",
        "    original_text.append(stories[i])\n",
        "    original_summary.append(summaries[i])\n",
        "    #t5\n",
        "    inputs_t5 = tokenizer_t5.encode(\"summarize: \" + stories[i], return_tensors=\"pt\", max_length=300, truncation=True)\n",
        "    outputs_t5 = model_t5.generate(\n",
        "        inputs_t5, \n",
        "        max_length=12, \n",
        "        min_length=3, \n",
        "        length_penalty=1.0,  \n",
        "        early_stopping=True)\n",
        "    created_summary_t5.append(tokenizer_t5.decode(outputs_t5[0]))\n",
        "    #bart\n",
        "    inputs_bart = tokenizer_bart.encode(stories[i], return_tensors=\"pt\", max_length=300, truncation=True)\n",
        "    outputs_bart = model_bart.generate(\n",
        "        inputs_bart, \n",
        "        max_length=12, \n",
        "        min_length=3, \n",
        "        length_penalty=1.0,  \n",
        "        early_stopping=True)\n",
        "    created_summary_bart.append(tokenizer_bart.decode(outputs_bart[0]))\n",
        "    "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ2YQxmMQoN-"
      },
      "source": [
        "results = pd.DataFrame()\n",
        "results[\"Original_text\"] = original_text\n",
        "results[\"Original_summary\"] = original_summary\n",
        "results[\"Created_summary_t5\"] = created_summary_t5\n",
        "results[\"Created_summary_bart\"] = created_summary_bart\n",
        "results[\"Created_summary_t5\"] = results[\"Created_summary_t5\"].str.replace(\"<pad> \", \"\")\n",
        "results[\"Created_summary_bart\"] = results[\"Created_summary_bart\"].str.replace(\"</s><s>\", \"\")\n",
        "results[\"Created_summary_bart\"] = results[\"Created_summary_bart\"].str.replace(\"</s>\", \"\")\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                             Original_text  \\\n",
              "0  former first lady barbara bush spend night houston texas hospital tuesday night falling ill according family spokeswoman former first lady barbara bush admitted houston texas hospital spokeswoman ...   \n",
              "1  saying one diminish team accomplished confident diana nyad took critics wednesday night piers morgan live record breaking cuba florida swin said book swam fair square squeaky clean across thing ny...   \n",
              "\n",
              "                                             Original_summary  \\\n",
              "0   new former first lady spend night houston texas hospital    \n",
              "1      diana nyad completed mile swim cuba florida last week    \n",
              "\n",
              "                      Created_summary_t5  \\\n",
              "0  barbara bush admitted to houston texa   \n",
              "1                 diana nyad says she is   \n",
              "\n",
              "                                    Created_summary_bart  \n",
              "0           former first lady barbara bush spend night h  \n",
              "1  saying one diminish team accomplished confident diana  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Original_text</th>\n      <th>Original_summary</th>\n      <th>Created_summary_t5</th>\n      <th>Created_summary_bart</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>former first lady barbara bush spend night houston texas hospital tuesday night falling ill according family spokeswoman former first lady barbara bush admitted houston texas hospital spokeswoman ...</td>\n      <td>new former first lady spend night houston texas hospital</td>\n      <td>barbara bush admitted to houston texa</td>\n      <td>former first lady barbara bush spend night h</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>saying one diminish team accomplished confident diana nyad took critics wednesday night piers morgan live record breaking cuba florida swin said book swam fair square squeaky clean across thing ny...</td>\n      <td>diana nyad completed mile swim cuba florida last week</td>\n      <td>diana nyad says she is</td>\n      <td>saying one diminish team accomplished confident diana</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-aN4j8-QoN-"
      },
      "source": [
        "results.to_csv(\"results_predictions_transformers.csv\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaW0LAO2QoN_"
      },
      "source": [
        "results=pd.read_csv(\"results_predictions_transformers.csv\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7erDJMdsQoOA"
      },
      "source": [
        "reference_sentences = results[\"Original_summary\"].to_list()\n",
        "summary_sentences_t5 = results[\"Created_summary_t5\"].to_list()\n",
        "summary_sentences_bart = results[\"Created_summary_bart\"].to_list()\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "source": [
        "# Evaluation"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5fwxZZQoOA",
        "outputId": "e52f0ec4-0e1b-4b84-912c-dc83eb40148c"
      },
      "source": [
        "from rouge import rouge_n_sentence_level\n",
        "from rouge import rouge_l_sentence_level\n",
        "from rouge import rouge_n_summary_level\n",
        "from rouge import rouge_l_summary_level\n",
        "from rouge import rouge_w_sentence_level\n",
        "from rouge import rouge_w_summary_level\n",
        "\n",
        "mean_rouge_r2_t5 = 0\n",
        "mean_rouge_r1_t5= 0\n",
        "mean_rouge_r2_bart= 0\n",
        "mean_rouge_r1_bart= 0\n",
        "\n",
        "summary_sentences_list = [summary_sentences_t5, summary_sentences_bart]\n",
        "mean_rouge_r2_list = [mean_rouge_r2_t5, mean_rouge_r2_bart]\n",
        "mean_rouge_r1_list = [mean_rouge_r1_t5, mean_rouge_r1_bart]\n",
        "\n",
        "for iteration in range(2):\n",
        "    print(\"Model: \", iteration)\n",
        "    \n",
        "    list_rouge_r2 = []\n",
        "    list_recall_r2 = []\n",
        "    list_precision_r2 = []\n",
        "    list_rouge_r1= []\n",
        "    list_recall_r1 = []\n",
        "    list_precision_r1 = []\n",
        "\n",
        "    for i in range(0, len(reference_sentences)):\n",
        "        clear_output(wait=True)\n",
        "        print(i)\n",
        "        reference_sentence = reference_sentences[i].split()\n",
        "        summary_sentence = summary_sentences_list[iteration][i].split()\n",
        "        \n",
        "        # Calculate ROUGE-2.\n",
        "        recall_r2, precision_r2, rouge_r2 = rouge_n_sentence_level(summary_sentence, reference_sentence, 2)\n",
        "\n",
        "        list_rouge_r2.append(rouge_r2)\n",
        "        list_recall_r2.append(recall_r2)\n",
        "        list_precision_r2.append(precision_r2)\n",
        "\n",
        "        # Calculate ROUGE-1.\n",
        "        recall_r1, precision_r1, rouge_r1 = rouge_n_sentence_level(summary_sentence, reference_sentence, 1)\n",
        "\n",
        "        list_rouge_r1.append(rouge_r1)\n",
        "        list_recall_r1.append(recall_r1)\n",
        "        list_precision_r1.append(precision_r1)\n",
        "\n",
        "    import statistics\n",
        "\n",
        "\n",
        "    mean_rouge_r2_list[iteration] = statistics.mean(list_rouge_r2)  \n",
        "    mean_rouge_r1_list[iteration] = statistics.mean(list_rouge_r1)  \n",
        "\n",
        "    print(\"Mean ROUGE-2 : \", mean_rouge_r2_list[iteration])\n",
        "    print(\"Mean ROUGE-1 : \", mean_rouge_r1_list[iteration])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  0\n0\n1\nMean ROUGE-2 :  0.08333333333333333\nMean ROUGE-1 :  0.2095238095238095\nModel:  1\n0\n1\nMean ROUGE-2 :  0.19999999999999998\nMean ROUGE-1 :  0.35661764705882354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}