{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('dsim': conda)",
      "metadata": {
        "interpreter": {
          "hash": "5db4922bf968224b7264651ce9cfe92d565015e9e8f9433dac9b1b05577b1c8f"
        }
      }
    },
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvkTTztiQoND"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBf9FBoWQoNQ"
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "from os import listdir\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np  \n",
        "import pandas as pd \n",
        "import re           \n",
        "from bs4 import BeautifulSoup \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords   \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from attention import AttentionLayer\n",
        "import warnings\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import pickle\n",
        "import inflect\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ThM5ZHZQoNU",
        "outputId": "d2589387-b2bd-4e4a-9a97-4603c5d3cc18"
      },
      "source": [
        "if tf.test.gpu_device_name(): \n",
        "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "   print(\"Please install GPU version of TF\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default GPU Device:/device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Load data"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ViAlTE7QoN0"
      },
      "source": [
        "x_test = np.load(\"../../../final_data/x_test.npy\")\n",
        "y_test = np.load(\"../../../final_data/y_test.npy\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAKGKdX1RWAf",
        "outputId": "6b4a76c4-d8eb-4c12-c031-0940a76a26a2"
      },
      "source": [
        "#!pip install pickle5\n",
        "import pickle"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdO9MW_xQoN1"
      },
      "source": [
        "def open_test_data_y():\n",
        "    return open('../../../tokenizers_vars/y_tokenizer.pickle', 'rb')\n",
        "\n",
        "with open_test_data_y() as f:\n",
        "    y_tokenizer = pickle.load(f) \n",
        "\n",
        "def open_test_data_x():\n",
        "    return open('../../../tokenizers_vars/x_tokenizer.pickle', 'rb')\n",
        "\n",
        "with open_test_data_x() as f:\n",
        "    x_tokenizer = pickle.load(f) \n",
        "\n",
        "def open_vars():\n",
        "    return open('../../../tokenizers_vars/vars.pkl', 'rb')\n",
        "\n",
        "with open_vars() as f:  # Python 3: open(..., 'rb')\n",
        "    x_voc, y_voc = pickle.load(f)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MmBzaOBQoN1"
      },
      "source": [
        "max_text_len=300\n",
        "max_summary_len=12"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGmvYD07QoN5"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwhZAzwiTVfN"
      },
      "source": [
        "model = tf.keras.models.load_model('saved_model')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEyKcrfXQoN5"
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vnHwOk5QoN6"
      },
      "source": [
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "\n",
        "# Encode the input sequence to get the feature vector\n",
        "encoder_inputs = model.input[0]   # input_1\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = model.layers[3].output \n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_inputs = model.input[1]\n",
        "decoder_state_input_h = Input(shape=(latent_dim*2,), name='dec_st_in_h')\n",
        "decoder_state_input_c = Input(shape=(latent_dim*2,), name='dec_st_in_c')\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim*2))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb_layer = model.layers[4]\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_lstm = model.layers[7]\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_layer = model.layers[8]\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_dense = model.layers[10]\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm0EESvyQoN6"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['starttoken']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='endtoken'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'endtoken'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEmOH4rcQoN7"
      },
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['starttoken']) and i!=target_word_index['endtoken']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAHdWMo9QoN8",
        "outputId": "bee271b3-256a-459f-f6e6-ed9d89a7c1bf"
      },
      "source": [
        "for i in range(0,1):\n",
        "    print(\"Review:\",seq2text(x_test[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_test[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(x_test[i].reshape(1,max_text_len)))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: movie cost million make winner obvious mark talks direct shining prequel apes strong start two weeks however maintain stand one stronger films summer friday seemed purge would take spoils title well movie earned million friday night showings compared apes lackluster million percent drop opening night week apes rallied big time saturday earning million purge million percent drop opening night writing wall apes slot went planes fire rescue studios sequel last year family comedy adventurous airplanes spun pixar cars universe million mil less first planes pretty bad stall family movie middle summer low friday gross million seemed like may simply reflection many adults turned opening night fare fire rescue much better family friendly matinee hours saturday earning million cinemascore rating turn enjoyed saw said cameron diaz jason segel sex tape nabbed dismal c plus cinemascore opened fourth place million diaz comedy hot streak lately raunchy slapstick couple trying prevent friends family seeing accidentally uploaded video seemed like would primed continue trend critical mass sex tape pay see bad teacher bad critics cruel audiences disagree director jake also helmed bad teacher quite seem know tone going last half movie wildly crude hard r comedy warm hearted teachable moments also short wrote ew leah greenblatt limited release zach partially kickstarter donation funded wish opened theaters earned relatively strong per screen eternal sunshine mind director michel whimsical french mood collected two locations average per screen philosophical sci fi saga origins earned four locations average persecuted paranoid thriller preacher framed murder straddled wide release limited release framework launching theaters earning per screen top five dawn planet apes million weekend purge anarchy million weekend planes fire rescue million weekend sex tape million weekend transformers age extinction million weekend see original story ew com click try risk free issues entertainment weekly entertainment weekly time inc rights reserved \n",
            "Original summary: dawn planet box office \n",
            "4\n",
            "29\n",
            "3\n",
            "2\n",
            "Predicted summary:  new old says\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQUkioFEQoN9"
      },
      "source": [
        "original_text = []\n",
        "original_summary = []\n",
        "created_summary = []\n",
        "\n",
        "for i in range(0,100):\n",
        "    original_text.append(seq2text(x_test[i]))\n",
        "    original_summary.append(seq2summary(y_test[i]))\n",
        "    created_summary.append(decode_sequence(x_test[i].reshape(1,max_text_len)))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ2YQxmMQoN-"
      },
      "source": [
        "results = pd.DataFrame()\n",
        "results[\"Original_text\"] = original_text\n",
        "results[\"Original_summary\"] = original_summary\n",
        "results[\"Created_summary\"] = created_summary"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-aN4j8-QoN-"
      },
      "source": [
        "results.to_csv(\"results_predictions_mono_10.csv\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaW0LAO2QoN_"
      },
      "source": [
        "results=pd.read_csv(\"results_predictions_mono_10.csv\")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7erDJMdsQoOA"
      },
      "source": [
        "reference_sentences = results[\"Original_summary\"].to_list()\n",
        "summary_sentences = results[\"Created_summary\"].to_list()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "source": [
        "# Evaluation"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5fwxZZQoOA",
        "outputId": "e52f0ec4-0e1b-4b84-912c-dc83eb40148c"
      },
      "source": [
        "from rouge import rouge_n_sentence_level\n",
        "from rouge import rouge_l_sentence_level\n",
        "from rouge import rouge_n_summary_level\n",
        "from rouge import rouge_l_summary_level\n",
        "from rouge import rouge_w_sentence_level\n",
        "from rouge import rouge_w_summary_level\n",
        "\n",
        "list_rouge = []\n",
        "list_recall = []\n",
        "list_precision = []\n",
        "\n",
        "for i in range(0, len(reference_sentences)):\n",
        "    reference_sentence = reference_sentences[i].split()\n",
        "    summary_sentence = summary_sentences[i].split()\n",
        "    \n",
        "    # Calculate ROUGE-2.\n",
        "    recall, precision, rouge = rouge_n_sentence_level(summary_sentence, reference_sentence, 2)\n",
        "    #print('ROUGE-2-R', recall)\n",
        "    #print('ROUGE-2-P', precision)\n",
        "    #print('ROUGE-2-F', rouge)\n",
        "\n",
        "    list_rouge.append(rouge)\n",
        "    list_recall.append(recall)\n",
        "    list_precision.append(precision)\n",
        "\n",
        "import statistics\n",
        "\n",
        "\n",
        "mean_rouge = statistics.mean(list_rouge)  \n",
        "print(mean_rouge)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.013205128205128204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwFnkYLhQoOB"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}