{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "5db4922bf968224b7264651ce9cfe92d565015e9e8f9433dac9b1b05577b1c8f"
        }
      }
    },
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvkTTztiQoND"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBf9FBoWQoNQ"
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "from os import listdir\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np  \n",
        "import pandas as pd \n",
        "from IPython.display import display, clear_output\n",
        "import re           \n",
        "from bs4 import BeautifulSoup \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords   \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from attention import AttentionLayer\n",
        "import warnings\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import pickle\n",
        "import inflect\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ThM5ZHZQoNU",
        "outputId": "d2589387-b2bd-4e4a-9a97-4603c5d3cc18"
      },
      "source": [
        "if tf.test.gpu_device_name(): \n",
        "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "   print(\"Please install GPU version of TF\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default GPU Device:/device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Load data"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ViAlTE7QoN0"
      },
      "source": [
        "x_test = np.load(\"../../../final_data/x_test.npy\")\n",
        "y_test = np.load(\"../../../final_data/y_test.npy\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAKGKdX1RWAf",
        "outputId": "6b4a76c4-d8eb-4c12-c031-0940a76a26a2"
      },
      "source": [
        "#!pip install pickle5\n",
        "import pickle"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdO9MW_xQoN1"
      },
      "source": [
        "def open_test_data_y():\n",
        "    return open('../../../tokenizers_vars/y_tokenizer.pickle', 'rb')\n",
        "\n",
        "with open_test_data_y() as f:\n",
        "    y_tokenizer = pickle.load(f) \n",
        "\n",
        "def open_test_data_x():\n",
        "    return open('../../../tokenizers_vars/x_tokenizer.pickle', 'rb')\n",
        "\n",
        "with open_test_data_x() as f:\n",
        "    x_tokenizer = pickle.load(f) \n",
        "\n",
        "def open_vars():\n",
        "    return open('../../../tokenizers_vars/vars.pkl', 'rb')\n",
        "\n",
        "with open_vars() as f:  # Python 3: open(..., 'rb')\n",
        "    x_voc, y_voc = pickle.load(f)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MmBzaOBQoN1"
      },
      "source": [
        "max_text_len=300\n",
        "max_summary_len=12"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGmvYD07QoN5"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwhZAzwiTVfN"
      },
      "source": [
        "model = tf.keras.models.load_model('saved_model')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEyKcrfXQoN5"
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vnHwOk5QoN6"
      },
      "source": [
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "\n",
        "# Encode the input sequence to get the feature vector\n",
        "encoder_inputs = model.input[0]   # input_1\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = model.layers[3].output \n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_inputs = model.input[1]\n",
        "decoder_state_input_h = Input(shape=(latent_dim*2,), name='dec_st_in_h')\n",
        "decoder_state_input_c = Input(shape=(latent_dim*2,), name='dec_st_in_c')\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim*2))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb_layer = model.layers[4]\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_lstm = model.layers[7]\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_layer = model.layers[8]\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_dense = model.layers[10]\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm0EESvyQoN6"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['starttoken']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='endtoken'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'endtoken'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEmOH4rcQoN7"
      },
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['starttoken']) and i!=target_word_index['endtoken']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAHdWMo9QoN8",
        "outputId": "bee271b3-256a-459f-f6e6-ed9d89a7c1bf"
      },
      "source": [
        "for i in range(0,1):\n",
        "    print(\"Review:\",seq2text(x_test[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_test[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(x_test[i].reshape(1,max_text_len)))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: promptly broken nato says simply true pledge made evidence back russia claims ever produced alliance wrote april fact sheet entitled russia accusations setting record straight nato says tried hard make russia privileged partner worked together russia range issues counter terrorism counter narcotics submarine rescue emergency planning nato says fundamentally russia anti nato rhetoric attempt divert attention away actions ukraine cooperation table russian side nato russian cooperation camouflage says vladimir russian think tank institute usa canada studies cold war russia tried several times become member americans always said going happen quotes lord nato first secretary general object nato existence keep russians americans germans russian president vladimir putin declared annual direct call russian people part reasoning annexing crimea protect sevastopol home russia black sea fleet ever falling nato hands anything ukraine drawn nato sometime future told concern nato ships dock sevastopol city russia naval glory said ukraine prime minister arseniy said ukrainian accession nato priority nation currently state disarray nato membership seems unimaginable membership action plan discussed ukraine georgia bucharest summit put hold putin forget ever since yanukovych fled country pro western government took power country course something putin stop thinking says lipman carnegie moscow center prevent ukraine becoming part western orbit nato something absolutely cannot afford rotation troops small baltic states poland joint training exercises affront russia perhaps strictly fair accuse russia engaging propaganda declares mistrust nato says feels general public attitude alliance worsened since end cold war people able dismiss kremlin line towards nato soviet propaganda says different store unsuccessful mishaps relations russia west end cold war contributed rise suspicions russian side western policy general nato particular one reasons putin popularity soared since annexation crimea feeling among general public last russia standing rights post soviet space sat maligned decades much kremlin likes nurture narrative also easy see resonates russian public \n",
            "Original summary: school russian questions nato \n",
            "Predicted summary:  russia russia russia ukraine\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQUkioFEQoN9"
      },
      "source": [
        "original_text = []\n",
        "original_summary = []\n",
        "created_summary = []\n",
        "\n",
        "for i in range(0,2000):\n",
        "    clear_output(wait=True)\n",
        "    print(i)\n",
        "    original_text.append(seq2text(x_test[i]))\n",
        "    original_summary.append(seq2summary(y_test[i]))\n",
        "    created_summary.append(decode_sequence(x_test[i].reshape(1,max_text_len)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ2YQxmMQoN-"
      },
      "source": [
        "results = pd.DataFrame()\n",
        "results[\"Original_text\"] = original_text\n",
        "results[\"Original_summary\"] = original_summary\n",
        "results[\"Created_summary\"] = created_summary"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-aN4j8-QoN-"
      },
      "source": [
        "results.to_csv(\"results_predictions_bi_10.csv\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaW0LAO2QoN_"
      },
      "source": [
        "results=pd.read_csv(\"results_predictions_bi_10.csv\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "results[\"Created_summary\"].replace(np.nan, 'NaN', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7erDJMdsQoOA"
      },
      "source": [
        "reference_sentences = results[\"Original_summary\"].to_list()\n",
        "summary_sentences = results[\"Created_summary\"].to_list()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "source": [
        "# Evaluation"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5fwxZZQoOA",
        "outputId": "e52f0ec4-0e1b-4b84-912c-dc83eb40148c"
      },
      "source": [
        "from rouge import rouge_n_sentence_level\n",
        "from rouge import rouge_l_sentence_level\n",
        "from rouge import rouge_n_summary_level\n",
        "from rouge import rouge_l_summary_level\n",
        "from rouge import rouge_w_sentence_level\n",
        "from rouge import rouge_w_summary_level\n",
        "\n",
        "list_rouge_r2 = []\n",
        "list_recall_r2 = []\n",
        "list_precision_r2 = []\n",
        "list_rouge_r1= []\n",
        "list_recall_r1 = []\n",
        "list_precision_r1 = []\n",
        "\n",
        "for i in range(0, len(reference_sentences)):\n",
        "    clear_output(wait=True)\n",
        "    print(i)\n",
        "    \n",
        "    reference_sentence = reference_sentences[i].split()\n",
        "    summary_sentence = summary_sentences[i].split()\n",
        "    \n",
        "    # Calculate ROUGE-2.\n",
        "    recall_r2, precision_r2, rouge_r2 = rouge_n_sentence_level(summary_sentence, reference_sentence, 2)\n",
        "\n",
        "    list_rouge_r2.append(rouge_r2)\n",
        "    list_recall_r2.append(recall_r2)\n",
        "    list_precision_r2.append(precision_r2)\n",
        "\n",
        "    # Calculate ROUGE-1.\n",
        "    recall_r1, precision_r1, rouge_r1 = rouge_n_sentence_level(summary_sentence, reference_sentence, 1)\n",
        "\n",
        "    list_rouge_r1.append(rouge_r1)\n",
        "    list_recall_r1.append(recall_r1)\n",
        "    list_precision_r1.append(precision_r1)\n",
        "\n",
        "import statistics\n",
        "\n",
        "\n",
        "mean_rouge_r2 = statistics.mean(list_rouge_r2)  \n",
        "mean_rouge_r1 = statistics.mean(list_rouge_r1)  \n",
        "\n",
        "print(\"Mean ROUGE-2: \", mean_rouge_r2)\n",
        "print(\"Mean ROUGE-1: \", mean_rouge_r1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999\nMean ROUGE-2:  0.006434259612936083\nMean ROUGE-1:  0.060007247297765874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}